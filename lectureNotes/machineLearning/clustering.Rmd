---
title: "Methods for Unsupervised Clustering"
author: | 
  | W. Evan Johnson, Ph.D.
  | Professor, Division of Infectious Disease
  | Director, Center for Data Science
  | Rutgers University -- New Jersey Medical School
date: "8/25/2022"
header-includes:
   - \usepackage{amsmath}
output: 
  beamer_presentation:
    theme: "CambridgeUS"
editor_options: 
  chunk_output_type: console
tables: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(tidyverse)
img_path <- "cluster/"
```

## Supervised vs. Unsupervised Machine Learning

Machine learning algorithms are generally classified into two categories. Most of the algorithms we have described up to now are examples of a general approach referred to as __supervised__ machine learning. The name comes from the fact that we use the outcomes in a training set to __supervise__ the creation of our prediction algorithm. 

There is another subset of machine learning methods referred to as __unsupervised__. In this subset we do not necessarily know the outcomes and instead are interested in discovering groups. These algorithms are also referred to as __clustering__ algorithms since predictors are used to define __clusters__. 

## Classification vs. Clustering
__Classification__ and __clustering__ are two methods of pattern identification used in machine learning. 

Although both techniques have certain similarities, the difference lies in the fact that _classification_ uses predefined classes in which objects are assigned, while _clustering_ identifies similarities between objects, which it groups according to those characteristics in common and which differentiate them from other groups of objects. These groups are known as "clusters".

## Supervised vs. Unsupervised Machine Learning
Sometimes clustering is not be very useful. For example, if we are simply given the heights we may not be able to discover two groups, males and females. The digits example would also be also be challenging:

\scriptsize
```{r mnist-27-unsupervised, message=FALSE, warning=FALSE, out.width="50%",fig.align='center'}
library(tidyverse)
library(dslabs)
data("mnist_27")
mnist_27$train %>% qplot(x_1, x_2, data = .)
```

## Unsupervised Machine Learning
However, there are applications in which unsupervised learning can be a powerful technique, in particular as an exploratory tool, and can be combined with multiple methods. 
\center
![](clusterFigs/singlecell_plot.jpg){width=60%}

## Unsupervised Machine Learning
There are many algorithms for unsupervised learning. We have already learned about __PCA__ and __UMAP__ for dimension reduction. Here we introduce two methods for clustering: __hierarchical clustering__ and __k-means__.


## Unsupervised Machine Learning and clustering
A first step in any clustering algorithm is defining a distance between observations or groups of observations. Then we need to decide how to join observations into clusters. 

__Hierarchical clustering__ starts by defining each observation as a separate group, and distances are calculated between every group (distance matrix). Then the two closest groups are merged into a single group, and this new group (two observations) is represented by its centroid. Distances between this new group and the rest are calculated, and then the next two closest groups are merged. This process is repeated until there is just one group including all the observations.

## Hierarchical clustering
Consider the ratings of 50 movies from 139 different critics: 
\scriptsize
```{r}
library(dslabs); data("movielens")
top <- movielens %>% group_by(movieId) %>%
  summarize(n=n(), title = first(title)) %>%
  top_n(50, n) %>% pull(movieId)

x <- movielens %>%filter(movieId %in% top) %>%
  group_by(userId) %>% filter(n() >= 25) %>%
  ungroup() %>% select(title, userId, rating) %>%
  spread(userId, rating)

row_names <- str_remove(x$title, ": Episode") %>% str_trunc(20)
x <- x[,-1] %>% as.matrix()
x <- sweep(x, 2, colMeans(x, na.rm = TRUE))
x <- sweep(x, 1, rowMeans(x, na.rm = TRUE))
rownames(x) <- row_names
```

## Hierarchical clustering
We want to use these data to find out if there are clusters of movies based on the ratings from  `r ncol(x)` movie raters. A first step is to find the distance between each pair of movies using the `dist` function: 

```{r}
d <- dist(x)
```

## Hierarchical clustering
With the distance between each pair of movies computed, we need an algorithm to define groups from these. The `hclust` function implements this algorithm and it takes a distance as input.

```{r}
h <- hclust(d)
h
```

## Hierarchical clustering
We can see the resulting groups using a __dendrogram__. 

```{r dendrogram, out.width="100%", fig.width = 8, fig.height = 5, echo=FALSE}
rafalib::mypar()
plot(h, cex = 0.65, main = "", xlab = "")
```

## Hierarchical clustering
This graph gives us an approximation between the distance between any two movies. To find this distance we find the first location, from top to bottom, where these movies split into two different groups. The height of this location is the distance between these two groups. So, for example, the distance between the three _Star Wars_ movies is 8 or less, while the distance between _Raiders of the Lost of Ark_ and _Silence of the Lambs_ is about 17.

## Hierarchical clustering
To generate actual groups: 1) decide on a maximum  distance to be in the same group or 2) decide on the number of groups. For example:

```{r, echo=F, out.width="100%", fig.width = 8, fig.height = 5, echo=FALSE}
rafalib::mypar()
plot(h, cex = 0.65, main = "", xlab = "")
abline(h=14,col=2)
```

## Hierarchical clustering
The function `cutree` can be applied to the output of `hclust` to perform either of these two operations and generate groups.
\small
```{r}
# Maximum Distance
groups <- cutree(h, h = 14)
table(groups)
#Number of groups
groups <- cutree(h, k = 10)
table(groups)
```

## Hierarchical clustering
The clustering provides some insights, e.g., Group 4 appears to be blockbusters:

\scriptsize
```{r}
names(groups)[groups==4]
```

\normalsize
And group 9 appears to be fantasy/nerd movies:

\scriptsize
```{r}
names(groups)[groups==9]
```

## Hierarchical clustering
We can also explore the data to see if there are clusters of movie raters.

```{r dendrogram-2, , out.width="100%", fig.height=5}
h_2 <- dist(t(x)) %>% hclust()
plot(h_2, cex = 0.35)
```


## k-means

We can also use the k-means algorithm. Here, we have to pre-define $k$, the number of clusters we want to define. 

The k-means algorithm is iterative. The first step is to define $k$ centers. Then each observation is assigned to the cluster with the closest center to that observation. In a second step the centers are redefined using the observation in each cluster: the column means are used to define a __centroid__. We repeat these two steps until the centers converge.

## k-means
The `kmeans` function included in R-base does not handle NAs. For illustrative purposes we will fill out the NAs with 0s. In general, the choice of how to fill in missing data, or if one should do it at all, should be made with care.

```{r}
x_0 <- x; x_0[is.na(x_0)] <- 0
set.seed(0)
k <- kmeans(x_0, centers = 10)
```

## k-means
The cluster assignments are in the `cluster` component:

```{r}
groups <- k$cluster
table(groups)
```

## k-means
This yields some interesting groups:
\scriptsize
```{r}
names(groups)[groups==4]
names(groups)[groups==6]
names(groups)[groups==7]
names(groups)[groups==9]
names(groups)[groups==10]
```

## k-means
Note that because the first center is chosen at random, the final clusters are random. We impose some stability by repeating the entire function several times and averaging the results. The number of random starting values to use can be assigned through the `nstart` argument.

```{r}
k <- kmeans(x_0, centers = 10, nstart = 25)
```


## Heatmaps

A powerful visualization tool for discovering clusters or patterns in your data is the heatmap. The idea is simple: plot an image of your data matrix with colors used as the visual cue and both the columns and rows ordered according to the results of a clustering algorithm. We will demonstrate this with the `tissue_gene_expression` dataset. We will scale the rows of the gene expression matrix.

The first step is compute: 
```{r}
data("tissue_gene_expression")
x <- sweep(tissue_gene_expression$x, 2, 
           colMeans(tissue_gene_expression$x))
h_1 <- hclust(dist(x))
h_2 <- hclust(dist(t(x)))
```

## Heatmaps
Now we can use the results of this clustering to order the rows and columns.

```{r heatmap, out.width="75%", fig.height=7, eval=TRUE,fig.align='center'}
image(x[h_1$order, h_2$order])
```

## Heatmaps
But there is `heatmap` function that does it for us:
\scriptsize
```{r heatmap-2, out.width="50%", fig.height=7, eval=TRUE,fig.align='center'}
heatmap(x, col = RColorBrewer::brewer.pal(11, "Spectral"))
```


## Filtering features

If the information about clusters in included in just a few features, including all the features can add enough noise that detecting clusters becomes challenging. One simple approach to try to remove features with no information is to only include those with high variance. In the movie example, a user with low variance in their ratings is not really informative: all the movies seem about the same to them. 


## Filtering features
For example, if we include only features (genes) with highest variance.

\scriptsize
```{r heatmap-3, out.width="40%", fig.height=5, fig.width=6, message=FALSE, warning=FALSE, fig.align='center'}
library(matrixStats)
sds <- colSds(x, na.rm = TRUE)
o <- order(sds, decreasing = TRUE)[1:25]
heatmap(x[,o], col = RColorBrewer::brewer.pal(11, "Spectral"))
```




## Session Info
\tiny
```{r session}
sessionInfo()
```
